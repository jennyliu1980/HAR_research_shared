============================================================
Self-supervised Pretraining for HAR
============================================================
Dataset: ucihar
Masking: time
  Time mask: 40%
============================================================

Loading ucihar dataset...
Data shape: (6172, 128, 9)
  Samples: 6172
  Timesteps: 128
  Features: 9
  Classes: 6

Initializing model...
  Total parameters: 466,569

Starting pretraining...
  Epochs: 150
  Batch size: 256
  Learning rate: 0.001
============================================================
Training on cpu
Total batches per epoch: 25
Epoch [1/150] - Loss: 0.7520 - Time Loss: 0.7520 - Channel Loss: 0.0000 - Time: 15.6s
Epoch [2/150] - Loss: 0.7129 - Time Loss: 0.7129 - Channel Loss: 0.0000 - Time: 16.4s
Epoch [3/150] - Loss: 0.6916 - Time Loss: 0.6916 - Channel Loss: 0.0000 - Time: 16.5s
Epoch [4/150] - Loss: 0.6922 - Time Loss: 0.6922 - Channel Loss: 0.0000 - Time: 17.0s
Epoch [5/150] - Loss: 0.6875 - Time Loss: 0.6875 - Channel Loss: 0.0000 - Time: 16.5s
Epoch [6/150] - Loss: 0.6840 - Time Loss: 0.6840 - Channel Loss: 0.0000 - Time: 16.7s
Epoch [7/150] - Loss: 0.6867 - Time Loss: 0.6867 - Channel Loss: 0.0000 - Time: 16.5s
Epoch [8/150] - Loss: 0.6903 - Time Loss: 0.6903 - Channel Loss: 0.0000 - Time: 16.7s
Epoch [9/150] - Loss: 0.6905 - Time Loss: 0.6905 - Channel Loss: 0.0000 - Time: 17.4s
Epoch [10/150] - Loss: 0.6834 - Time Loss: 0.6834 - Channel Loss: 0.0000 - Time: 17.0s
Epoch [11/150] - Loss: 0.6845 - Time Loss: 0.6845 - Channel Loss: 0.0000 - Time: 16.6s
Epoch [12/150] - Loss: 0.6919 - Time Loss: 0.6919 - Channel Loss: 0.0000 - Time: 17.2s
Epoch [13/150] - Loss: 0.6812 - Time Loss: 0.6812 - Channel Loss: 0.0000 - Time: 17.1s
Epoch [14/150] - Loss: 0.6867 - Time Loss: 0.6867 - Channel Loss: 0.0000 - Time: 17.0s
Epoch [15/150] - Loss: 0.6774 - Time Loss: 0.6774 - Channel Loss: 0.0000 - Time: 17.8s
Epoch [16/150] - Loss: 0.6769 - Time Loss: 0.6769 - Channel Loss: 0.0000 - Time: 17.4s
Epoch [17/150] - Loss: 0.6866 - Time Loss: 0.6866 - Channel Loss: 0.0000 - Time: 18.2s
Epoch [18/150] - Loss: 0.6781 - Time Loss: 0.6781 - Channel Loss: 0.0000 - Time: 18.1s
Epoch [19/150] - Loss: 0.6809 - Time Loss: 0.6809 - Channel Loss: 0.0000 - Time: 16.8s
Epoch [20/150] - Loss: 0.6716 - Time Loss: 0.6716 - Channel Loss: 0.0000 - Time: 17.4s
Epoch [21/150] - Loss: 0.6699 - Time Loss: 0.6699 - Channel Loss: 0.0000 - Time: 16.6s
Epoch [22/150] - Loss: 0.6665 - Time Loss: 0.6665 - Channel Loss: 0.0000 - Time: 18.2s
Epoch [23/150] - Loss: 0.6584 - Time Loss: 0.6584 - Channel Loss: 0.0000 - Time: 18.4s
Epoch [24/150] - Loss: 0.6709 - Time Loss: 0.6709 - Channel Loss: 0.0000 - Time: 17.7s
Epoch [25/150] - Loss: 0.6613 - Time Loss: 0.6613 - Channel Loss: 0.0000 - Time: 17.3s
Epoch [26/150] - Loss: 0.6531 - Time Loss: 0.6531 - Channel Loss: 0.0000 - Time: 18.4s
Epoch [27/150] - Loss: 0.6518 - Time Loss: 0.6518 - Channel Loss: 0.0000 - Time: 19.5s
Epoch [28/150] - Loss: 0.6580 - Time Loss: 0.6580 - Channel Loss: 0.0000 - Time: 16.8s
Epoch [29/150] - Loss: 0.6430 - Time Loss: 0.6430 - Channel Loss: 0.0000 - Time: 15.6s
Epoch [30/150] - Loss: 0.6429 - Time Loss: 0.6429 - Channel Loss: 0.0000 - Time: 18.3s
Epoch [31/150] - Loss: 0.6404 - Time Loss: 0.6404 - Channel Loss: 0.0000 - Time: 18.5s
Epoch [32/150] - Loss: 0.6338 - Time Loss: 0.6338 - Channel Loss: 0.0000 - Time: 19.2s
Epoch [33/150] - Loss: 0.6216 - Time Loss: 0.6216 - Channel Loss: 0.0000 - Time: 19.9s
Epoch [34/150] - Loss: 0.6129 - Time Loss: 0.6129 - Channel Loss: 0.0000 - Time: 26.5s
Traceback (most recent call last):
  File "main_wandb.py", line 292, in <module>
    best_loss, best_epoch = pretrain_with_wandb(model, args.dataset, x_train, args)
  File "main_wandb.py", line 156, in pretrain_with_wandb
    loss, time_loss, channel_loss = train_step(
  File "main_wandb.py", line 65, in train_step
    out = model(x)
  File "C:\Users\jenny\miniconda3\envs\maskcae\lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\jenny\miniconda3\envs\maskcae\lib\site-packages\torch\nn\modules\module.py", line 1603, in _call_impl
    result = forward_call(*args, **kwargs)
  File "C:\Users\jenny\OneDrive\Desktop\PythonProject_HAR\module.py", line 86, in forward
    encoded = self.encoder(x)  # (batch_size, seq_len, d_model)
  File "C:\Users\jenny\miniconda3\envs\maskcae\lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\jenny\miniconda3\envs\maskcae\lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\jenny\OneDrive\Desktop\PythonProject_HAR\encoder.py", line 42, in forward
    x = self.enc_layers[i](x, mask)
  File "C:\Users\jenny\miniconda3\envs\maskcae\lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\jenny\miniconda3\envs\maskcae\lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\jenny\OneDrive\Desktop\PythonProject_HAR\encoderLayer.py", line 34, in forward
    ffn_output = self.ffn(out1)  # (batch_size, seq_len, d_model)
  File "C:\Users\jenny\miniconda3\envs\maskcae\lib\site-packages\torch\nn\modules\module.py", line 1716, in __getattr__
    def __getattr__(self, name: str) -> Any:
KeyboardInterrupt
