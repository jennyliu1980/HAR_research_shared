============================================================
Self-supervised Pretraining for HAR
============================================================
Dataset: ucihar
Masking: spantime
  Time mask: 10%
============================================================

Loading ucihar dataset...
Data shape: (6172, 128, 9)
  Samples: 6172
  Timesteps: 128
  Features: 9
  Classes: 6

Initializing model...
  Total parameters: 466,569

Starting pretraining...
  Epochs: 150
  Batch size: 256
  Learning rate: 0.001
============================================================
Training on cpu
Total batches per epoch: 25
Traceback (most recent call last):
  File "main_wandb.py", line 292, in <module>
    best_loss, best_epoch = pretrain_with_wandb(model, args.dataset, x_train, args)
  File "main_wandb.py", line 156, in pretrain_with_wandb
    loss, time_loss, channel_loss = train_step(
  File "main_wandb.py", line 65, in train_step
    out = model(x)
  File "C:\Users\jenny\miniconda3\envs\maskcae\lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\jenny\miniconda3\envs\maskcae\lib\site-packages\torch\nn\modules\module.py", line 1603, in _call_impl
    result = forward_call(*args, **kwargs)
  File "C:\Users\jenny\OneDrive\Desktop\PythonProject_HAR\module.py", line 86, in forward
    encoded = self.encoder(x)  # (batch_size, seq_len, d_model)
  File "C:\Users\jenny\miniconda3\envs\maskcae\lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\jenny\miniconda3\envs\maskcae\lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\jenny\OneDrive\Desktop\PythonProject_HAR\encoder.py", line 42, in forward
    x = self.enc_layers[i](x, mask)
  File "C:\Users\jenny\miniconda3\envs\maskcae\lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\jenny\miniconda3\envs\maskcae\lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\jenny\OneDrive\Desktop\PythonProject_HAR\encoderLayer.py", line 30, in forward
    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, seq_len, d_model)
  File "C:\Users\jenny\miniconda3\envs\maskcae\lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\jenny\miniconda3\envs\maskcae\lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\jenny\OneDrive\Desktop\PythonProject_HAR\multiHeadAttention.py", line 56, in forward
    scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)
  File "C:\Users\jenny\OneDrive\Desktop\PythonProject_HAR\multiHeadAttention.py", line 19, in scaled_dot_product_attention
    output = torch.matmul(attention_weights, v)  # (batch_size, num_heads, seq_len_q, depth_v)
KeyboardInterrupt
